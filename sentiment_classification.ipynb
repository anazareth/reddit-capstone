{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "training_raw = pd.read_csv(r\"C:\\Users\\zande\\Documents\\Ryerson\\Capstone\\Data\\bow_train.csv\", encoding = \"utf-8\", low_memory=False)\n",
    "test_raw = pd.read_csv(r\"C:\\Users\\zande\\Documents\\Ryerson\\Capstone\\Data\\bow_test.csv\", encoding = \"utf-8\", low_memory=False)\n",
    "\n",
    "training_data = training_raw.loc[:,[\"title\", \"sentiment\"]]\n",
    "testing_data = test_raw.loc[:,[\"title\", \"sentiment\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer        \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(words, stemmer):\n",
    "    stemmed = []\n",
    "    for w in words:\n",
    "        stemmed.append(stemmer.stem(w))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_words(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word', tokenizer = tokenize, lowercase = True,\n",
    "                             stop_words = 'english', max_features = 1000)  # top 1000 features by frequency\n",
    "# want all possible words in our corpus (labeled/train and unlabeled/test)\n",
    "corpus_all = vectorizer.fit_transform(list(training_data[\"title\"]) + list(testing_data[\"title\"])).toarray()\n",
    "\n",
    "corpus_vocab = vectorizer.get_feature_names() # all words\n",
    "\n",
    "dist = np.sum(corpus_all, axis=0) # freq of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zande\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.83      0.81      1198\n",
      "          1       0.83      0.79      0.81      1241\n",
      "\n",
      "avg / total       0.81      0.81      0.81      2439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(corpus_all[:len(training_data)], training_data[\"sentiment\"],\n",
    "                                                     train_size=0.90, random_state=416)\n",
    "\n",
    "log_reg_model = LogisticRegression()\n",
    "log_reg_model = log_reg_model.fit(X=X_train, y=y_train)\n",
    "y_pred = log_reg_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.87      0.87       430\n",
      "          1       0.87      0.87      0.87       430\n",
      "\n",
      "avg / total       0.87      0.87      0.87       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg_model_2 = LogisticRegression()\n",
    "log_reg_model_2.fit(X = corpus_all[:len(training_data)], y=training_data[\"sentiment\"])\n",
    "\n",
    "test_pred = log_reg_model_2.predict(corpus_all[len(training_data):])\n",
    "test_labels = list(testing_data[\"sentiment\"])\n",
    "print(classification_report(test_pred, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
